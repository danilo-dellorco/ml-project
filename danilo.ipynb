{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Introduzione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'obiettivo è quello di utilizzare il dataset [Patient Survival Prediction](https://www.kaggle.com/datasets/mitishaagarwal/patient) per effettuare il training diversi classificatori, al fine di predirre se un paziente sopravvivrà o morirà sulla base dei *parametri vitali* registrati al momento dell'ingresso in ospedale. Per fare ciò si effettua innanzitutto una fase di **Data Exploration**, in modo da ricavare informazioni utili sui dati da utilizzare, e comprendere quali sono le feature probabilmente più rilevanti ai fini della classificazione. Dopo aver splittato il dataset in **Testing e Training Set**, si procede conj l'**Ingegneria delle Features** sul Training Set, rimuovendo le features con *valori nulli*, *poco correlate* con il target, ed applicando la *feature selection*. Successivamente si esegue una fase di **Model Selection** sui vari classificatori considerati, effettuando il *tuning dei loro iperparametri*. Infine, si eseguono tutti i **Classificatori** con le features ed i parametri individuati in precedenza, effettuando un confronto tra le prestazioni ottenute.\n",
    "\n",
    "Il notebook è composto come segue:\n",
    "\n",
    "- Operazioni Preliminari\n",
    "- Data Exploration\n",
    "    - Descrizione del Dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Operazioni Preliminari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriviamo le operazioni preliminari, necessarie per l'esecuzione delle varie celle nel notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installazione Dipendenze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di eseguire l'import delle librerie, è necessario che tutte queste siano installate sulla macchina locale. Per questo nel file `requirements` sono specificate tutte le dipendenze necessarie, facilmente installabili lanciando dalla directory del progetto il comando:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip install -r requirements\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import delle Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caricamento del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caricamento del dataset `csv` in `Dataframe` tramite `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset/dataset.csv\", sep=\",\")\n",
    "dataset_bak = dataset\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni e variabili user-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effettuiamo la definizione di tutte le funzioni utilizzate nel notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global trainX                   # Righe del training set\n",
    "global testX                    # Righe del testing set\n",
    "global trainY                   # Colonna target del training set\n",
    "global testY                    # Colonna target del testing set\n",
    "global important_features       # Lista delle feature individuate nell'analisi preliminare\n",
    "global correlation_matrix       # Matrice di correlazione tra le features\n",
    "\n",
    "global trainX_og                # Mantiene una copia del training set originale, per il confronto delle prestazioni dopo la feature selection\n",
    "global testX_og                 # Mantiene una copia del testing set originale, per il confronto delle prestazioni dopo la feature selection\n",
    "global scores_dict              # Dizionario che mantiene tutti i risultati delle classificazioni effettuate\n",
    "\n",
    "global scores_original          # Dataframe che mantiene i risultati della classificazioni sul dataset originale\n",
    "global scores                   # Dataframe che mantiene i risultati delle classificazioni dopo la feature selection\n",
    "\n",
    "\n",
    "important_features = []\n",
    "scores_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Rimuove le features columns_list specificate dal training set\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def drop_features_from_trainset(columns_list):\n",
    "    global trainX,testX\n",
    "    trainX_tmp = trainX.copy()\n",
    "    trainX = trainX.drop(columns=columns_list)\n",
    "    testX = testX.drop(columns=columns_list)\n",
    "\n",
    "    if (len(trainX_tmp.columns) - len(trainX.columns)) != 0:\n",
    "        print (\"Training Set Updated:\",trainX_tmp.shape,\"->\",trainX.shape)\n",
    "    else:\n",
    "        print (\"No features dropped!\")\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Ottiene la lista di features con correlazione rispetto al target inferiore alla threshold specificata\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_low_correlated_features(threshold):\n",
    "    td = []\n",
    "    for feature in trainX.columns:\n",
    "        ind = trainX.columns.get_loc(feature)\n",
    "        target_corr = correlation_matrix.values[-1][ind]\n",
    "        if (target_corr>=-threshold and target_corr<0) or (target_corr<=threshold and target_corr>0):\n",
    "            td.append(feature)\n",
    "    return td\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Ottiene la percentuale di valori nulli per ogni feature del dataset, ritornando quelle colonne cui %NaN supera una certa threshold\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def plot_nas(datasetX, use_threshold, threshold=None):\n",
    "    to_delete = []\n",
    "    if datasetX.isnull().sum().sum() != 0:\n",
    "        na_datasetX = (datasetX.isnull().sum() / len(datasetX)) * 100\n",
    "        na_datasetX = na_datasetX.drop(na_datasetX[na_datasetX == 0].index).sort_values(ascending=False)\n",
    "        to_delete = na_datasetX[na_datasetX > threshold]\n",
    "\n",
    "        missing_data = pd.DataFrame({'Null Ratio %' :na_datasetX})\n",
    "        missing_data.plot(kind = \"barh\")\n",
    "        plt.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "        if use_threshold:\n",
    "            plt.axvline(x=threshold, color='red')\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print('No NaNs found')\n",
    "        return None\n",
    "    \n",
    "    if use_threshold:\n",
    "        print(to_delete.index.tolist())\n",
    "    return to_delete.index.tolist()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Mostra la matrice di confusione a seguito della classificazione\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def show_confusion_matrix(cm,model_name):\n",
    "    x_axis_labels = [\"Dead\", \"Survived\"]\n",
    "    y_axis_labels = [\"Dead\", \"Survived\"]\n",
    "    f, ax = plt.subplots(figsize =(7,7))\n",
    "    sns.heatmap(cm, annot = True, linewidths=0.2, linecolor=\"black\", fmt = \".0f\", ax=ax, cmap=\"Reds\", xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n",
    "    plt.xlabel(\"PREDICTED LABEL\")\n",
    "    plt.ylabel(\"TRUE LABEL\")\n",
    "    plt.title('Confusion Matrix for {}'.format(model_name))\n",
    "    plt.show()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Aggiunge i risultati di un classificatore al dizionario globale dei risultati.\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def add_score(model_name,experiment,conf_matrix, report):\n",
    "    TP,FN,FP,TN = conf_matrix.ravel()\n",
    "    \n",
    "    scores_dict[model_name] = {\n",
    "        'Precision' : report['1']['precision'],\n",
    "        'Accuracy' : report['accuracy'],\n",
    "\n",
    "        'TPR (recall)': TP/(TP+FN),\n",
    "        'FNR': FN/(TP+FN),   \n",
    "        'TNR': TN/(TN+FP),\n",
    "        'FPR': FP/(TN+FP),\n",
    "\n",
    "        'experiment' : experiment\n",
    "    }\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Esegue i tre classificatori usati per il benchmark, oppure un modello specifico se indicato esplicitamente tra i parametri.\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def run_classifications(tr_x,train_Y,te_x,test_Y,fill=None,model=None,model_name=None,experiment=None,show_report=False):\n",
    "    train_X = tr_x.copy()\n",
    "    test_X = te_x.copy()\n",
    "\n",
    "    if fill:\n",
    "        train_X = train_X.fillna(-1)\n",
    "    \n",
    "    test_X = test_X.fillna(-1)\n",
    "    \n",
    "    if model == None:\n",
    "        lrc = LogisticRegression(solver=\"liblinear\")\n",
    "        rdt = DecisionTreeClassifier()\n",
    "        gnb = GaussianNB()\n",
    "\n",
    "        models = [(lrc, \"Logistic Regression Classifier\"),\n",
    "                  (rdt, \"Decision Tree Classifier\"),\n",
    "                  (gnb, \"Gaussian Naive Bayes Classifier\")]\n",
    "    else:\n",
    "        models = [(model,model_name)]\n",
    "\n",
    "    for model in models:\n",
    "        model[0].fit(train_X, train_Y)\n",
    "        predicted = model[0].predict(test_X)\n",
    "        report = classification_report(test_Y,predicted,output_dict=True)\n",
    "        cm = confusion_matrix(test_Y, predicted, labels =[1, 0])\n",
    "        add_score(model[1],experiment,cm,report)\n",
    "        if show_report:\n",
    "            report_print = classification_report(test_Y,predicted)\n",
    "            print(model[1], \"report: \\n\\n\", report_print)\n",
    "            show_confusion_matrix(cm,model[1])\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Esegue la feature selection tramite Backward Regression, ritornando le colonne rimosse dalle features\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def backward_regression(X, y, threshold_out=0.05):\n",
    "    included = list(X.columns)\n",
    "    recover = []\n",
    "    removed = []\n",
    "    \n",
    "    while True:\n",
    "        changed = False\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max()  # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed = True\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            if worst_feature not in important_features:\n",
    "                print(f\"worst_feature : {worst_feature}, {worst_pval} \")\n",
    "                removed.append(worst_feature)\n",
    "            else:\n",
    "                recover.append(worst_feature)\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    included.extend(recover)\n",
    "    print(f\"\\nSelected Features:\\n{included}\")\n",
    "    print(f\"Removed Features:\\n{removed}\")\n",
    "    return removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella fase di data exploration andiamo ad analizzare il dataset a disposizione, cercando di ricavare informazioni utili e migliorare la conoscenza sul dominio di interesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrizione del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset riporta parametri vitali di $91713$ pazienti al momento dell'ingresso in ospedale, come ad esempio peso, età, presenza di malattie gravi, o vari indici di [Apache Scoring](https://pubmed.ncbi.nlm.nih.gov/11579607/). Per ogni paziente, viene inoltre riportato se al termine del trattamento questo è sopravvissuto o deceduto. L'obiettivo dei classificatori allenati su questo dataset, è quindi quello di predirre in base ai parametri di un paziente se questo è in pericolo di vita oppure no.\n",
    "\n",
    "La classe target è dunque ```hospital_death```, che è rappresentata con un valore binario:\n",
    "- `0` se il paziente è sopravvissuto.\n",
    "- `1` se il paziente è deceduto.\n",
    "\n",
    "Si considera come **positivo** la classe `1`, in quanto si è più interessati a predirre se un paziente morirà . Considerando il dominio di interesse, si assume quindi che un *falso negativo sia un errore più grave rispetto ad un falso positivo*. Questo perché in uno scenario reale risulta molto più pericoloso sottostimare il problema (paziente grave, ma si predice che sopravvivrà), rispetto all'essere più pessimisti (paziente che sopravvive, ma si predice che non sopravvirà)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Class Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo visualizzare il livello di bilanciamento del dataset rispetto alla classe target `hospital_death`. Utilizziamo quindi un `pieplot` per graficare il totale di istanze positive e negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [{\"Survived\": (dataset[\"hospital_death\"] == 0).sum(), \"Dead\": (dataset[\"hospital_death\"] == 1).sum()}]\n",
    "total  = pd.DataFrame(classes)\n",
    "total_e = float(total[\"Survived\"])\n",
    "total_p = float(total[\"Dead\"])\n",
    "patients = [total_e, total_p]\n",
    "patients_labels = 'Survived','Dead'\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "plt.pie(patients,labels=patients_labels,autopct='%1.1f%%',colors = ['#88d14f', '#f23d3a'], explode=[0.05,0.05])\n",
    "plt.title('Dataset Balancing', loc = \"center\", fontsize=\"20\")\n",
    "plt.axis('equal')\n",
    "plt.legend(patients_labels,bbox_to_anchor=(0.6, -0.05, 0, 0))\n",
    "fig.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere il dataset è **altamente sbilanciato**. Infatti abbiamo il $91.4%$ di righe che riportano dati per un paziente sopravvissuto, mentre solo l'$8.6%$ di dati per un paziente deceduto. Ciò implica dire che qualsiasi modello allenato su tale dataset tenderà maggiormente a classificare un'istanza come `survived`, aspettandoci quindi un maggior numero di falsi negativi (`death` classificati erroneamente come `survived`).\n",
    "\n",
    "Occorre osservare come tale sbilanciamento non è dovuto ad una scarsa qualità del dataset o mancanza di dati raccolti, ma intrinseco nella natura del problema analizzato; infatti considerando un insieme di ospedale è atteso che il numero di pazienti che sopravvivono sia di molto superiore al numero di decessi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Distribuzione delle Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a visualizzare la distribuzione delle features all'interno del dataset, riportando in un barplot le variabili categoriche e in un histplot le variabili numeriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['xkcd:pale orange', 'xkcd:sea blue', 'xkcd:pale red', 'xkcd:sage green', 'xkcd:terra cotta', 'xkcd:dull purple', 'xkcd:teal', 'xkcd:goldenrod', 'xkcd:cadet blue', \n",
    "          'xkcd:scarlet']\n",
    "ignore = ['encounter_id','patient_id','Unnamed: 83','hospital_id','icu_id','hospital_death']\n",
    "categ = []\n",
    "numer = []\n",
    "\n",
    "for col in dataset.drop(columns=ignore): \n",
    "    dt = dataset[col].dtype\n",
    "    if dt == \"object\":\n",
    "        categ.append(col)\n",
    "    elif dt == \"float64\":\n",
    "        if dataset[col].dropna().nunique()<10:\n",
    "            dataset[col] = dataset[col].astype('object')\n",
    "            categ.append(col)\n",
    "        else:\n",
    "            numer.append(col)\n",
    "    elif dt == \"int64\":\n",
    "        dataset[col] = dataset[col].astype('object')\n",
    "        categ.append(col)\n",
    "\n",
    "fig = plt.figure(figsize=(120,120))\n",
    "j=1\n",
    "a=0\n",
    "b=0\n",
    "for i in range(0, len(dataset.columns)-len(ignore)):\n",
    "    col = dataset.columns[i]\n",
    "\n",
    "    if col in categ:\n",
    "        fig.add_subplot(10,10,j)\n",
    "        ax = sns.countplot(x=categ[a], data=dataset, alpha=.7, order=sorted(dataset[col].dropna().unique().tolist(),key=str))\n",
    "        ax.bar_label(ax.containers[0])\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
    "        j += 1\n",
    "        a += 1\n",
    "\n",
    "    elif col in numer:\n",
    "        fig.add_subplot(10,10,j)\n",
    "        sns.histplot(dataset[numer[b]].dropna(), kde_kws={\"lw\": 2, \"color\":colors[8]})\n",
    "        j += 1\n",
    "        b += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo osservare come la maggior parte delle variabili numeriche abbiano una distribuzione che tende ad una normale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualizzazione campi non nulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a visualizzare la percentuale di campi nulli presenti per ogni colonna *(feature)* del dataset. Questo risulta utile visualizzare eventuali colonne con pochi dati utili,e che possono quindi essere escluse ai fini del training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_nas(dataset,use_threshold=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzando la distribuzione delle varie features possiamo vedere come il dataset sia di buona qualità, avendo pochi campi non nulli nelle varie colonne. L'unica eccezione riguarda la feature ```Unnamed: 83```, che presenta esclusivamente campi nulli, e quindi verrà eliminata in una successiva fase di pulizia del dataset.\n",
    "\n",
    "Le restanti features presentano valori `NaN` per una percentuale sempre inferiore al 20%, motivo per cui possiamo procedere con il *filling* o l'*imputazione* di tali dati, senza introdurre un rumore eccessivo nel dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversione Features e Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si effettua la conversione delle features categoriche in ordinali, mappando ogni ```object``` identificato da una *stringa*, in un altro ```object``` identificato da un *numero*. Il dizionario ```mapping``` mantiene la corrispondenza tra label originale e numero assegnato, per un eventuale utilizzo futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset = dataset.copy()\n",
    "encoder = LabelEncoder()\n",
    "mapping = []\n",
    "\n",
    "for i in range(len(mapped_dataset.columns)):\n",
    "    \n",
    "    if (mapped_dataset[mapped_dataset.columns[i]].dtype == \"object\"):\n",
    "            mapped_dataset[mapped_dataset.columns[i]] = encoder.fit_transform(mapped_dataset[mapped_dataset.columns[i]])\n",
    "            mapping_dict = {index : label for index , label in enumerate(encoder.classes_)}\n",
    "            mapping.append(mapping_dict)\n",
    "\n",
    "mapped_dataset = mapped_dataset.where(~dataset.isna(), dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo il mapping effettuato delle variabili categoriche.\n",
    "\n",
    "- *NOTA*: I valori `NaN` risultano mappati nel dizionario, ma tramite la clausola `where(~dataset.isna(), dataset)` vengono ristabiliti i valori `NaN` presenti nel dataset originale, per la successiva fase di imputazione dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completiamo il mapping del dataset trasformando il tipo di dato associato alle features categoriche dall' `object` numerico al valore `float64` ad esso associato, in quanto i vari classificatori operano comunque con valori numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in mapped_dataset.columns:\n",
    "    if (mapped_dataset[el].dtype == \"object\"):\n",
    "        mapped_dataset[el] = np.floor(pd.to_numeric(mapped_dataset[el], errors='coerce')).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che abbiamo mappato le nostre variabili categoriche in variabili numeriche. I valori nulli sono stati mantenuti <NaN> per una successiva fase di imputazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo il grado di correlazione reciproco tra le varie feature, compresa la correlazione di ogni feature con il target. In generale vogliamo che una feature sia **altamente correlata** con il target (quindi che il suo valore sia importante per discriminare se il paziente sopravvive o muore). Si utilizza a tale scopo l'indice di Pearson, in cui si ha un valore compreso tra $(-1,+1)$:\n",
    "- $-1$ corrisponde alla perfetta correlazione lineare positiva.\n",
    "- $0$ corrisponde a un'assenza di correlazione lineare.\n",
    "- $-1$ corrisponde alla perfetta correlazione lineare negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = mapped_dataset.drop(columns=\"Unnamed: 83\").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostriamo la matrice di correlazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,24), dpi = 250)\n",
    "sns.heatmap(correlation_matrix, linewidths=0.05, cmap=\"PiYG\", annot=True, annot_kws={\"fontsize\":4})\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi delle Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo l'impatto di alcune features rispetto al valore del target. In questo modo dall'analisi del dominio si possono individuare le features che risultano rilevanti ai fini della classificazione, e che vogliamo necessariamente mantenere anche a seguito della riduzione della dimensionalità."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità di morte in relazione all'età"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo la probabilità di decesso in relazione all'età dei pazienti, distinguendo due casi in base al genere `maschio` o `femmina`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "age_death_F=dataset[dataset['gender']=='F'][['age','hospital_death']].groupby('age').mean().reset_index()\n",
    "age_death_M=dataset[dataset['gender']=='M'][['age','hospital_death']].groupby('age').mean().reset_index()\n",
    "fig,ax = plt.subplots(figsize = (12,8))\n",
    "sns.lineplot(x=age_death_F['age'], y=age_death_F['hospital_death'])\n",
    "sns.lineplot(x=age_death_M['age'], y=age_death_M['hospital_death'])\n",
    "plt.title(\"Average hospital death probability of patients\", fontsize = 20)\n",
    "\n",
    "plt.legend(['Female',\"Male\"])\n",
    "plt.xlabel(\"Patient Age\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "fig.show()\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come atteso, l'età risulta essere un fattore determinante nella mortalità dei pazienti ricoverati, ed infatti si può notare un incremento della mortalità con l'avanzare dell'età. Inoltre si può anche osservare come il genere non risulta rilevante nella probabilità di morte, in quanti maschi e femmine hanno circa lo stesso andamento in relazione all'età."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.append('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità di morte in relazione all'etnia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo verificare se l'etnia di un paziente ha un impatto sulla probabilità di morte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df=dataset[['hospital_death','ethnicity']]\n",
    "\n",
    "ethnicity_death=df[['ethnicity','hospital_death']].groupby('ethnicity').mean().reset_index()\n",
    "fig,ax = plt.subplots(figsize = (12,8))\n",
    "sns.barplot(x=ethnicity_death['ethnicity'], y=ethnicity_death['hospital_death'])\n",
    "plt.title(\"Impacts of Ethnicity over patients\", fontsize = 20)\n",
    "\n",
    "plt.xlabel(\"ethnicity\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "fig.show()\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo vedere la probabilità di morte risulta simile tra tutte le etnie. La feature `ethnicity` non è quindi da considerarsi importante ai fini della classificazione, e verrà ulteriormente analizzata nella fase di *feature engeneering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità di morte in relazione al BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo visualizzare l'impatto dell'*Indice di Massa Corporea* sulla probabilità dei pazienti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df=dataset[['hospital_death','bmi']]\n",
    "\n",
    "df['bmi'] = df['bmi'].round(0)\n",
    "bmi_death=df[['bmi','hospital_death']].groupby('bmi').mean().reset_index()\n",
    "fig,ax = plt.subplots(figsize = (12,8))\n",
    "sns.lineplot(x=bmi_death['bmi'], y=bmi_death['hospital_death'])\n",
    "plt.title(\"Impacts of BMI over patients\", fontsize = 20)\n",
    "\n",
    "plt.xlabel(\"BMI\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "fig.show()\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il BMI impatta notevolmente nella mortalità, infatti un valore troppo basso ($<20$, gravemente sottopeso) o troppo alto ($>50$, obesità di terzo grado) aumenta la probabilità di decesso.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.append('bmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità di morte in relazione a score apache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli score apache misurano parametri vitali critici, quindi che discriminano fortemente la probabilità di sopravvivenza di un paziente. Andiamo a graficare quindi l'impatto di tali indici sulla probabilità di decesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "critical = ['hospital_death','gcs_eyes_apache','gcs_motor_apache','gcs_unable_apache','gcs_verbal_apache','apache_3j_bodysystem','apache_2_bodysystem']\n",
    "df=dataset[critical]\n",
    "\n",
    "gcs_eyes_apache_death=df[['gcs_eyes_apache','hospital_death']].groupby('gcs_eyes_apache').mean().reset_index()\n",
    "gcs_motor_apache_death=df[['gcs_motor_apache','hospital_death']].groupby('gcs_motor_apache').mean().reset_index()\n",
    "gcs_unable_apache_death=df[['gcs_unable_apache','hospital_death']].groupby('gcs_unable_apache').mean().reset_index()\n",
    "gcs_verbal_apache_death=df[['gcs_verbal_apache','hospital_death']].groupby('gcs_verbal_apache').mean().reset_index()\n",
    "apache_3j_bodysystem_death=df[['apache_3j_bodysystem','hospital_death']].groupby('apache_3j_bodysystem').mean().reset_index()\n",
    "apache_2_bodysystem_death=df[['apache_2_bodysystem','hospital_death']].groupby('apache_2_bodysystem').mean().reset_index()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "fig.add_subplot(2,3,1)\n",
    "sns.barplot(x=gcs_eyes_apache_death['gcs_eyes_apache'], y=gcs_eyes_apache_death['hospital_death'])\n",
    "plt.title(\"Impacts of GCS Eyes over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"GCS Eyes\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(2,3,2)\n",
    "sns.barplot(x=gcs_motor_apache_death['gcs_motor_apache'], y=gcs_motor_apache_death['hospital_death'])\n",
    "plt.title(\"Impacts of GCS Motor over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"GCS Motor\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(2,3,3)\n",
    "sns.barplot(x=gcs_unable_apache_death['gcs_unable_apache'], y=gcs_unable_apache_death['hospital_death'])\n",
    "plt.title(\"Impacts of GCS Unable over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"GCS Unable\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(2,3,4)\n",
    "sns.barplot(x=gcs_verbal_apache_death['gcs_verbal_apache'], y=gcs_verbal_apache_death['hospital_death'])\n",
    "plt.title(\"Impacts of GCS Verbal over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Hepatitic Failure\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(2,3,5)\n",
    "ax = sns.barplot(x=apache_3j_bodysystem_death['apache_3j_bodysystem'], y=apache_3j_bodysystem_death['hospital_death'])\n",
    "plt.title(\"Impacts of Apache III over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"3J Bodysystem\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
    "\n",
    "fig.add_subplot(2,3,6)\n",
    "ax = sns.barplot(x=apache_2_bodysystem_death['apache_2_bodysystem'], y=apache_2_bodysystem_death['hospital_death'])\n",
    "plt.title(\"Impacts of Apache II over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"2 Bodysystem\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation = 45)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come nel caso degli score GCS, per gli score critici (valori più bassi) si hanno sempre probabilità più alte di decesso. Invece per gli indici di Bodysystem possiamo vedere come diagnosi differenti abbiano probabilità di decesso differenti. Ad esempio la setticemia o complicanze respiratorie sono più critiche rispetto a casi ginecologici o metabolici.\n",
    "\n",
    "Questo ci indica dunque come gli score Apache risultano critici ai fini della classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.extend(['gcs_eyes_apache','gcs_motor_apache','gcs_unable_apache','gcs_verbal_apache','apache_3j_bodysystem','apache_2_bodysystem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilità di morte in relazione a malattie critiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo l'impatto di differenti malattie critiche rispetto alla probabilità di decesso di un paziente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "critical = ['hospital_death','aids','cirrhosis','diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']\n",
    "df=dataset[critical]\n",
    "\n",
    "aids_death=df[['aids','hospital_death']].groupby('aids').mean().reset_index()\n",
    "cirrhosis_death=df[['cirrhosis','hospital_death']].groupby('cirrhosis').mean().reset_index()\n",
    "diabetes_mellitus_death=df[['diabetes_mellitus','hospital_death']].groupby('diabetes_mellitus').mean().reset_index()\n",
    "hepatic_failure_death=df[['hepatic_failure','hospital_death']].groupby('hepatic_failure').mean().reset_index()\n",
    "immunosuppression_death=df[['immunosuppression','hospital_death']].groupby('immunosuppression').mean().reset_index()\n",
    "leukemia_death=df[['leukemia','hospital_death']].groupby('leukemia').mean().reset_index()\n",
    "lymphoma_death=df[['lymphoma','hospital_death']].groupby('lymphoma').mean().reset_index()\n",
    "solid_tumor_death=df[['solid_tumor_with_metastasis','hospital_death']].groupby('solid_tumor_with_metastasis').mean().reset_index()\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "\n",
    "fig.add_subplot(3,3,1)\n",
    "sns.barplot(x=aids_death['aids'], y=aids_death['hospital_death'])\n",
    "plt.title(\"Impacts of AIDS over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"AIDS\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,2)\n",
    "sns.barplot(x=cirrhosis_death['cirrhosis'], y=cirrhosis_death['hospital_death'])\n",
    "plt.title(\"Impacts of Cirrhosis over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Cirrhosis\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,3)\n",
    "sns.barplot(x=diabetes_mellitus_death['diabetes_mellitus'], y=diabetes_mellitus_death['hospital_death'])\n",
    "plt.title(\"Impacts of Diabetes Mellitus over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Diabetes Mellitus\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,4)\n",
    "sns.barplot(x=hepatic_failure_death['hepatic_failure'], y=hepatic_failure_death['hospital_death'])\n",
    "plt.title(\"Impacts of Hepatitic Failure over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Hepatitic Failure\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,5)\n",
    "sns.barplot(x=immunosuppression_death['immunosuppression'], y=immunosuppression_death['hospital_death'])\n",
    "plt.title(\"Impacts of Immunosuppression over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Immunosuppression\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,6)\n",
    "sns.barplot(x=leukemia_death['leukemia'], y=leukemia_death['hospital_death'])\n",
    "plt.title(\"Impacts of Leukemia over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Lekuemia\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,7)\n",
    "sns.barplot(x=lymphoma_death['lymphoma'], y=lymphoma_death['hospital_death'])\n",
    "plt.title(\"Impacts of Lymphoma over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Lypmhoma\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.add_subplot(3,3,8)\n",
    "sns.barplot(x=solid_tumor_death['solid_tumor_with_metastasis'], y=solid_tumor_death['hospital_death'])\n",
    "plt.title(\"Impacts of Solid Tumor with Metastasis over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Solid Tumor with Metastasis\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come tutte queste variabili risultano importanti, in quanto incrementano la probabilità di decesso. L'unica eccezione è il diabete, che risulta non impattare nella probabilità di decesso (bassa in entrambi i casi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.extend(['aids','cirrhosis','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo se diverse strutture ospedaliere hanno maggiore o minore impatto sulla mortalità dei pazienti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(35,35))\n",
    "hospital_death=dataset[['hospital_id','hospital_death']].groupby('hospital_id').mean().reset_index()\n",
    "sns.barplot(x=hospital_death['hospital_id'], y=hospital_death['hospital_death'])\n",
    "plt.title(\"Impacts of Hospital over Patients\", fontsize = 20)\n",
    "plt.xlabel(\"Hospital\")\n",
    "plt.ylabel(\"Average Hospital Death\")\n",
    "plt.tick_params(labelsize = 8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che alcuni ospedali sembrano avere probabilità di decesso elevate. Ad esempio l'ospedale ```131``` ha il 50% di decessi. Analizzando però il dataset possiamo vedere come in realtà ci siano soltanto due istanze relative all'ospedale 130, che è quindi un outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['hospital_id'].value_counts()[130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche l'ospedale ```51``` ha una probabilità più alta di decesso. Ma anche in questo caso possiamo vedere come siano pochi i dati (110) rispetto agli altri ospedali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['hospital_id'].value_counts()[51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche la feature `hospital` non è quindi da considerarsi importante ai fini della classificazione, e verrà ulteriormente analizzata nella fase di *feature engeneering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Individuate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In definitiva sono ritenute di particolare interesse nella classificazione le seguenti features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training & Test Set Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a splittare il dataset in training e stesting set. Utilizziamo come percentuali 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = mapped_dataset['hospital_death']\n",
    "trainX, testX, trainY, testY = train_test_split(mapped_dataset.drop(columns = 'hospital_death'), targets, test_size = 0.2, random_state=0)\n",
    "print(\"TrainingSet:\",trainX.shape)\n",
    "print(\"TestingSet:\",testX.shape)\n",
    "\n",
    "trainX_og = trainX.copy()\n",
    "testX_og = testX.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scelta della Metrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come spiegato nella Descrizione del Dataset, si è maggiormente interessati ad **evitare i falsi negativi**, ovvero istanze predete come `survived` ma che sono in realtà `death`. Di conseguenza, la principale metrica considerata per valutare le prestazioni dei vari modelli è la <b>recall</b>: $$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Tuttavia, si deve tener conto anche dei **FP**, valutando un tradeoff per evitare un aumento eccessivo degli errori di classificazione sui pazienti `survived`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results on Original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a misurare le prestazioni di vari classificatori sul dataset originale, quindi senza ingegneria delle features e imputazione dei valori mancanti. \n",
    "- I classificatori considerati non gestiscono in modo automatico i valori ```NaN```\n",
    "- Per eseguire la classificazione è comunque necessario mappare i valori ```NaN``` su un qualche valore numerico\n",
    "- Si utilizza per questa fase di benchmark l'operatore ```fillna(-1)``` sostituendo quindi ```NaN``` con ```-1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX,trainY,testX,testY, fill = True, experiment=\"Original Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores_dict).T\n",
    "original_scores = scores.copy()\n",
    "original_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Features Tweaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad eseguire diverse operazioni per ridurre la dimensionalità del dataset, rimuovendo quelle features non rilevanti ai fini della classificazione.\n",
    "- Rimozione feature con valori nulli\n",
    "- Rimozione feature con bassa varianza dei dati\n",
    "- Rimozione feature con bassa correlazione con il target \n",
    "- Feature Selection\n",
    "- Altri tentativi di miglioramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rimozione feature con valori nulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo quelle colonne il cui valore percentuale di colonne nulle supera l'80%, e su cui risulterebbe inutile l'imputazione dei dati mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = plot_nas(trainX,use_threshold=True,threshold=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soltanto la feature `Unnamed: 83` supera la soglia prefissata, per cui la rimuoviamo dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features_from_trainset(features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rimozione features con bassa varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otteniamo la lista delle feature che presentano una varianza minore ad 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "original_cols = trainX_og.columns\n",
    "threshold = 0.01\n",
    "\n",
    "selector = VarianceThreshold(threshold)\n",
    "selector.fit(trainX)\n",
    "features_to_remove = [x for x in trainX.columns if x not in selector.get_feature_names_out()]\n",
    "print(\"Features to remove:\",features_to_remove)\n",
    "\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche se poco variabili, 'gcs_unable_apache', 'aids', 'apache_2_bodysystem', 'lymphoma' sono importanti come visto nell'analisi delle feature quindi non andiamo a rimuoverle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non viene rimossa nessuna feature dal training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = list(set(features_to_remove) - set(important_features))\n",
    "drop_features_from_trainset(features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Rimozione features con bassa correlazione col target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricaviamo le features che hanno una bassa correlazione con il target, inferiore ad una threshold di 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = get_low_correlated_features(0.03)\n",
    "print(features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di queste variabili aids, leukemia e lymphoma sono comunque importanti nel dominio quindi non le andiamo a rimuovere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = list(set(features_to_remove) - set(important_features))\n",
    "print(to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo le feature individuate dal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features_from_trainset(to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguiamo la Backward Regression, considerando anche le analisi preliminari sull'importanza delle features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = backward_regression(trainX.fillna(-1),trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo dal training set le feature non selezionate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features_from_trainset(features_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che al termine della fase di pulizia abbiamo ridotto notevolmente la dimensionalità del dataset, passando da 84 a 46 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto Prestazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo le prestazioni ottenute nel training set originale con quelle a seguito della *dimensionality reduction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX,trainY,testX,testY, fill = True, experiment=\"Feature Engeneering\")\n",
    "scores = pd.DataFrame(scores_dict).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo inoltre che mantenendo le **feature importanti** individuate nella fase di analisi, si hanno prestazioni sensibilmente migliori nel classificare le classi `death` rispetto al rimuoverle nel filtraggio automatico con varianza, correlazione e feature selection. In particolare si hanno delle classificazioni migliori in relazione alle classi `death`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Altri tentativi di Miglioramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo ulteriori tecniche applicate per migliorare la qualità del dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizza un meccanismo di imputazione per sopperire alla presenza di dati mancanti delle diverse features del dataset. L'obiettivo è quello di ripristinare le diverse variabili per l'uso nella classificazione rimpiazzando tutti i dati nulli con valori derivati da entry simili nel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = KNNImputer(n_neighbors=1, weights='uniform')\n",
    "trainX_imp = pd.DataFrame(imp.fit_transform(trainX), columns = trainX.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo i risultati a seguito della data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX_imp,trainY,testX,testY, fill = False)\n",
    "scores = pd.DataFrame(scores_dict).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che tutti gli score peggiorano. Questo perché si introduce comunque rumore nel dataset, e bisogna considerare che un valore assente potrebbe avere comunque un significato, e potrebbe esse in alcuni casi errato andarlo a rimpiazzare. Utilizzando invece `fillna(-1)` si mantengono i `NaN` semplicemente mappandoli a `-1`, valore esclusivo non usato per altre variabili nel dizionario. \n",
    "\n",
    "Possiamo quindi in definitiva mappare a -1 tutti i valori `NaN` nel training set e testing set. Si noti che la modifica nel testing set non rappresenta una manipolazione dei dati, ma soltanto come questi vengono codificati in fase di lettura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.fillna(-1)\n",
    "testX = testX.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo di ridurre ulteriormente la dimensionalità del dataset applicando la Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "pca = PCA(n_components=int(trainX.shape[1]*0.7))\n",
    "trainX_pca = pd.DataFrame(pca.fit_transform(trainX))\n",
    "testX_pca = pd.DataFrame(pca.transform(testX))\n",
    "print(trainX_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizziamo i risultati a seguito di PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX_pca,trainY,testX_pca,testY, experiment=\"PCA after FE\")\n",
    "scores_pca = pd.DataFrame(scores_dict).T\n",
    "scores_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I risultati risultano inferiori a seguito dell'applicazione di PCA sull'ultimo insieme di features individuate. Proviamo anche ad applicare PCA sull'insieme originale di features, per verificare se questo operi meglio rispetto al processo di selezione applicato. Anche in questo caso vengono generate 32 features sintetiche. Possiamo però osservare come i risultati siano addirittura peggiori rispetto al caso precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX_og.shape)\n",
    "pca = PCA(n_components=int(trainX.shape[1]*0.7))\n",
    "trainX_pca = pd.DataFrame(pca.fit_transform(trainX_og.fillna(-1)))\n",
    "testX_pca = pd.DataFrame(pca.transform(testX_og.fillna(-1)))\n",
    "print(trainX_pca.shape)\n",
    "\n",
    "run_classifications(trainX_pca,trainY,testX_pca,testY,experiment=\"PCA without FE\")\n",
    "scores_pca = pd.DataFrame(scores_dict).T\n",
    "scores_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In definitiva non viene applicata PCA in quanto si hanno risultati migliori applicando soltanto l'ingegneria delle features descritta in precedenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come visto nella Data Exploration il dataset risulta fortemente sbilanciato. Anche a seguito dello splitting in training e testing set resta tale sbilanciamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [{\"Survived\": (trainY== 0).sum(), \"Dead\": (trainY== 1).sum()}]\n",
    "total  = pd.DataFrame(classes)\n",
    "total_e = float(total[\"Survived\"])\n",
    "total_p = float(total[\"Dead\"])\n",
    "mushrooms = [total_e, total_p]\n",
    "mushrooms_labels = 'Survived','Dead'\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "plt.pie(mushrooms,labels=mushrooms,autopct='%1.1f%%',colors = ['#88d14f', '#f23d3a'])\n",
    "plt.title('Dataset Balancing', loc = \"center\", fontsize=\"20\")\n",
    "plt.axis('equal')\n",
    "plt.legend(mushrooms_labels,bbox_to_anchor=(0.6, -0.05, 0, 0))\n",
    "fig.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo, come già visto nell'analisi preliminare porta il modello a classificare più frequentemente le classi come `survived`, in quanto ha molti più dati in percentuale su cui effettuare il training. Per questo si possono applicare diverse strategie per cercare di ripristinare il bilanciamento del dataset:\n",
    "\n",
    "- Undersampling\n",
    "- Oversampling\n",
    "- SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_survd = (trainY== 0).sum()\n",
    "num_death = (trainY== 1).sum()\n",
    "\n",
    "num_under = num_survd - num_death\n",
    "print (\"Removing\",num_under,\"istances\")\n",
    "\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "trainX_und,trainY_und = rus.fit_resample(trainX_og.fillna(-1),trainY.fillna(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo dai risultati che peggiorano tutte le metriche applicando il balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX_und,trainY_und,testX_og,testY,experiment=\"undersampling\")\n",
    "scores_und = pd.DataFrame(scores_dict).T\n",
    "scores_und"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come atteso migliorano effettivamente le classificazioni di classi `death` rispetto a `survived`. Tuttavia rimuovendo $60882$ istanze dal training set si ha una significativa perdita di informazioni, che porta l'**accuracy** del modello a decrescere significativamente. Inoltre possiamo vedere all'aumento della recall corrisponda un abbassamento della precision, indice del fatto che il classificatore tende semplicemente a classificare più istanze come positive (aumentano i falsi negativi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_survd = (trainY== 0).sum()\n",
    "num_death = (trainY== 1).sum()\n",
    "\n",
    "num_under = num_survd - num_death\n",
    "print (\"Creating\",num_under,\"istances\")\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "trainX_ov,trainY_ov = ros.fit_resample(trainX_og.fillna(-1),trainY.fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifications(trainX_ov,trainY_ov,testX_og,testY,experiment=\"oversampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ov = pd.DataFrame(scores_dict).T\n",
    "scores_ov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=0)\n",
    "trainX_smote, trainY_smote = smote.fit_resample(trainX.fillna(-1), trainY)\n",
    "\n",
    "run_classifications(trainX_smote,trainY_smote,testX,testY,fill=True,experiment=\"SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_smote = pd.DataFrame(scores_dict).T\n",
    "scores_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tra tutti i metodi di sampling **SMOTE** è quello che fornisce risultati migliori, andando a creare istanze artificiali comunque simili a quelle già presenti. Come possiamo osservare, applicando il sampling si nota effettivamente un miglioramento nella classificazione delle classi `death`.\n",
    "\n",
    "Tuttavia è necessario considerare vengono inserite $\\frac{6}{7}$ di istanze artificiali. Lo sbilanciamento del dataset in questo caso risulta troppo elevato per poter applicare un metodo di sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad effettuare la fase di **model selection**, per stabilire i migliori iperparametri su ognuno dei modelli considerati ai fini della classificazione. \n",
    "\n",
    "Questa viene effettuata in generale sfruttando la ricerca esaustiva tramite `GridSearchCV`. Il miglior estimatore viene individuato considerando la metrica di ```recall``` su un'esecuzione **5Fold-Cross-Validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si considera l'ottimizzazione dei seguenti iperparametri:\n",
    "- `penalty` = `{l1,l2}`, regolarizzazione *lasso* o *ridge*\n",
    "- `C`, *regularization strenght*, ovvero il coefficiente di regolarizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'penalty':['l1','l2'], 'C':[0.1, 1, 10]}\n",
    "lrc = LogisticRegression(solver=\"liblinear\")\n",
    "gscv_lr = GridSearchCV(lrc, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "gscv_lr.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono selezionati i seguenti iperparametri:\n",
    "- `penalty = l1`\n",
    "- `C = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_lr.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo ci dice che non viene di fatto applicata la regolarizzazione, che peggiora i risultati rispetto al considerare l'intero dominio del training set. Questo vuol dire che il modello considerato non soffre in realtà di problemi di Overfitting, e quindi andandone a limitare il dominio si rimuovono informazioni utili al classificatore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I risultati ottenuti dalla model selection vedono ottenere una recall maggiore utilizzando come funzione di misurazione della qualità di ogni splitting la funzione entropy. Come atteso infatti, il Gini Index viene utilizzato più frequentemente in campo economico per misurare la disuguaglianza economica in una società, mentre in questi approcci si utilizza spesso l'entropia, che misura la goodness tramite il guadagno informativo.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random'], 'min_samples_split':[2, 3, 5], 'max_depth':[None, 10]}\n",
    "dtc = DecisionTreeClassifier()\n",
    "gscv_dtc = GridSearchCV(dtc, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "\n",
    "gscv_dtc.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che vengono trovati i segunenti parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_dtc.best_estimator_)\n",
    "print(\"Best Params: \", gscv_dtc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un classificatore bayesiano richiede la *conoscenza delle probabilità a priori*.  Un iperparametro tra i classificatori naive bayes è la distribuzione a priori da considerare, e come abbiamo visto dall'analisi delle features, la maggior parte di esse presentano una distribuzione simile ad una Gaussiana.\n",
    "\n",
    "Verifichiamo quindi i risultati ottenuti assumendo una distribuzione a priori Gaussiana, e confrontiamoli con il caso di un prior Bernoulliano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "gnb = GaussianNB()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, model=gnb, model_name=\"Naive Bayes Classifier (1)\", experiment = \"Gaussian Prior\")\n",
    "\n",
    "bbn = BernoulliNB()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, model=bbn, model_name=\"Naive Bayes Classifier (2)\", experiment = \"Bernoullian Prior\")\n",
    "\n",
    "scores_nb = pd.DataFrame(scores_dict).T\n",
    "scores_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dai risultati possiamo osservare come la recall sia nettamente superiore nel caso della distribuzione gaussiana ($0.55$ vs $0.14$), a fronte di un TNR che decresce solo del $9$.\n",
    "\n",
    "I parametri della distribuzione a priori utilizzata su ogni classe vengono automaticamente dedotti dal modello *sklearn*, scegliendo i valori di *media* e *varianza* più adatti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il **Random Forest** è un metodo bagging tra i più utilizzati in machine learning, che utilizza come weak-learner degli alberi di decisione.\n",
    "- Tali alberi di decisione sono diversi tra loro perché allenati su training set differenti, ottenuti tramite Boostrap.\n",
    "- Sono detti Random perché l’insieme delle feature rispetto a cui decidere se tagliare lo spazio è in realtà un sottoinsieme casuale delle feature.\n",
    "- Tale randomizzazione contribuisce a rendere diversi tra loro i vari classificatori.\n",
    "\n",
    "Dopo aver costruito $𝑇_𝐵$ alberi di decisione, la predizione finale viene effettuata per maggioranza (in caso di classificazione) o tramite la media dei risultati (in caso di regressione) su tutte le predizioni effettuate da ogni albero.\n",
    "\n",
    "Avendo già individuato i migliori iperparametri per il singolo weak learner, si fissano i valori:\n",
    "- `criterion = entropy`\n",
    "- `min_samples_split = 2`\n",
    "- `max_depth = None`\n",
    "\n",
    "Pertanto l'unico parametro da valutare è  `n_estimators`, ovvero il numero di Decision Trees da considerare nella foresta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[10,100,1000]}\n",
    "rfc = RandomForestClassifier(criterion='entropy',min_samples_split=2,max_depth=None)\n",
    "gscv_dtc = GridSearchCV(rfc, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "gscv_dtc.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I migliori iperaparametri ricavati sono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_dtc.best_estimator_)\n",
    "print(\"Best Params: \", gscv_dtc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo quindi con 1000 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(criterion='entropy',min_samples_split=2,max_depth=None, n_estimators=1000)\n",
    "run_classifications(trainX,trainY,testX,testY,model=rfc,model_name=\"Random Forest w/ 1000 estimators\",fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict={}\n",
    "scores_rf = pd.DataFrame(scores_dict).T\n",
    "scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I risultati risultano sensibilmente migliori in termine di precision e di recall. Tuttavia occorre considerare che la complessità computazionale aumenta di dieci volte *(6 min vs 21 sec)*, motivo per cui si mantiene il parametro standard di `n_estimators` pari a `100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un iperparametro (non so se è iperparametro) delle SVM è la funzione Kernel che andrà ad utilizzare nell'individuazione dell'iperpiano di separazione. La principali funzioni kernel messe a disposizione dal framework sklearn sono:\n",
    "- `lineare`\n",
    "- `polimoniale`\n",
    "- `rbf`\n",
    "- `sigmoide`\n",
    "\n",
    "Nel nostro caso, utilizzando un kernel `lineare` non viene raggiunta una convergenza e di conseguenza la SVM classificherà tutti gli elementi come `survived`.  \n",
    "Utilizzando il kernel `rbf` otteniamo un elevato score di accuracy, 0.92. Tuttavia, osservando i risultati ottenuti sulla classe 1, si ottiene un basso valore di recall pari a 0.20 ad indicare il fatto che vengono ancora classificati molti pazienti survived come death.   \n",
    "Stessa valutazione è possibile fare nel caso in cui venga utilizzato un kernel `polimoniale` che riesce ad ottenere solamente una recall del 0.23.  \n",
    "\n",
    "Anche se le classificazioni riportano valori alti di precision, siamo più interessati ad ottenere buoni valori di recall poichè la nostra preoccupazione principale è evitare di sovrastimare il numero di paziente predetti come survived.  \n",
    "Sotto questo punto di vista i risultati migliori li ottiene il kernel `sigmoid` il quale, anche se ottiene un valore non altissimo di recall sulla classe 1, 0.27, il numero dei falsi negativi risulta minore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "trainX_scaled = pd.DataFrame(scaler.fit_transform(trainX.fillna(-1)),columns = trainX.columns)\n",
    "testX_scaled = pd.DataFrame(scaler.transform(testX.fillna(-1)),columns = testX.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='poly')\n",
    "run_classifications(trainX_scaled, trainY, testX_scaled, testY, model=svc_classifier, model_name='SVC_POLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_poly = pd.DataFrame(scores_dict).T\n",
    "scores_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='rbf')\n",
    "run_classifications(trainX_scaled, trainY, testX_scaled, testY, model=svc_classifier, model_name='SVC_RBF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rbf = pd.DataFrame(scores_dict).T\n",
    "scores_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='sigmoid')\n",
    "run_classifications(trainX_scaled, trainY, testX_scaled, testY, model=svc_classifier, model_name='SVC_SIGMOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_sigmoid = pd.DataFrame(scores_dict).T\n",
    "scores_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaboost** è un algoritmo di *ensemble learning* che utilizza il metodo *Boosting*, ovvero i classificatori vengono addestrati in sequenza, ognuno dei quali costruito sulla base del comportamento del precedente.  \n",
    "Come classificatore base, sul quale viene applicato il metodo di boosting, è stato scelto di utilizzare il `base estimator` offerto di default, ovvero un *DecisionTree Classifier* di `max_depth=1`.  Ulteriori iper parametri da stimare sono:\n",
    "- Numero massimo di `estimators` che il boosting dovrà raggiungere\n",
    "- `learning_rate`, un peso applicato ad ogniclassificatore. Più tale valore è alto più aumenta il contributo apportato da ogni singolo classificatore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': [50, 100], 'learning_rate':[0.01,0.1,1,2]}\n",
    "\n",
    "adb = AdaBoostClassifier()\n",
    "gscv_adb = GridSearchCV(adb, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "\n",
    "gscv_adb.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come mostrato di seguito, il risultato migliore si raggiunge utilizzando:\n",
    "- `n_estimators = 50`\n",
    "- `learning_rate = 2`\n",
    "\n",
    "Testando valori maggiori di `learning _rate` otteniamo una altissima recall ma una precision quasi nulla, in quanto il classifiacatore costruito tenderà a classificare qualsiasi elemento come `death`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_adb.best_estimator_)\n",
    "print(\"Best Params: \", gscv_adb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(learning_rate=2)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=abc, model_name=\"AdaBoost Classifier\", experiment=\"default depth\")\n",
    "\n",
    "scores_ada = pd.DataFrame(scores_dict).T\n",
    "scores_ada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che si ha uno score bassissimo di `TNR` e un'altissima `FPR`, per cui il classificatore tende a predirre tutte le istanze come `dead`. Questo è probabilmente dovuto alla profondità minima dell'albero di decisione utilizzato come *base estimators*. \n",
    "\n",
    "Infatti, utilizzando sempre un `DecisionTreeClassifier` ma con l'iperparametro `max_depth = None` come visto nella relativa fase di Model Selection, otteniamo dei risultati migliori in termini di `precision` e `accuracy`, accettando però una maggiore presenza di `FN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decTree = DecisionTreeClassifier(max_depth=None)\n",
    "abc = AdaBoostClassifier(base_estimator = decTree, learning_rate=2)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True,  model=abc, model_name=\"AdaBoost Classifier\", experiment = \"unlimited depth\")\n",
    "\n",
    "scores_ada2 = pd.DataFrame(scores_dict).T\n",
    "scores_ada2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il `Linear Discriminant Analysis` di `scikit-learn` effettua una classificazione tramite decision boundaries lineari, dopo aver effettuato una riduzione della dimensionalità.  \n",
    "La ricerca dei miglior parametri si basa solamente sul tipo di `solver` da utilizzare:\n",
    "- `solver`:`[svd, lsqr, eigen]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'solver': ['svd', 'lsqr', 'eigen']}\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "gscv_lda = GridSearchCV(lda, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "\n",
    "gscv_lda.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ricerca individua in `svd` il miglior solver, che riesce ad ottenere una `accuracy` pari a `0.91` ma con un valore basso di `recall` pari a `0.32`, classificando infatti `1133 FN` su un totale di `1671` elementi positivi del testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_lda.best_estimator_)\n",
    "print(\"Best Params: \", gscv_lda.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNeighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il classificatore `k-Nearest-Neighbors` classifica un elemento in base ad un indice di somiglianza che è solitamente rappresentato da una metrica di distanza. Per determinare la classifincazione finale utilizza un voto di maggioranza tra  gli elementi più vicinial nuovo elemento da classificare. Ad esempio, se vicino all'elemento da classificare sono presenti 3 punti *blu* ed 1 punto *rosso*, l'elemento sarà classificato come blu.  \n",
    "I principali iper-parametri da determinare durante la fase di model selection sono:\n",
    "- `n_neighbors` : `[3, 5, 7]` \n",
    "- Indice di somgilianza degli elementi, `weights`: `[uniform, distance]`  \n",
    "\n",
    "La scelta dell'algoritmo utilizzato dal classificatore per l'individuazione dei `k` vicini è stata lasciata al classificatore stesso, dichiarando  `algorithm = auto`. Questo perchè, non conoscendo dettagliatamente l'implementazione degli algoritmi proposti da `scikit-learn`, il classificatore deciderà l'algoritmo più appropriato da utilizzare basandosi sui dati con cui è stato addestrato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'weights': ['uniform', 'distance'], 'n_neighbors': [3, 5, 7]}\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "gscv_knn = GridSearchCV(knc, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "\n",
    "gscv_knn.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli iper parametri che ottengono un miglior valore di `recall` sono:\n",
    "- `n_neighbors = 3`\n",
    "- `weights = distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_knn.best_estimator_)\n",
    "print(\"Best Params: \", gscv_knn.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La rete neurale implementata è una *Multilayer Perceptron* i quali iperparametri più importanti risultano essere:\n",
    "- `hidden_layer_size`: lista di interi in cui l'i-esimo elemento rappresenta il numero di neuroni che compongono l'i-esimo hidden layer\n",
    "- La *funzione di attivazione* degli hidden layer\n",
    "- `learning_rate_init` : il tasso iniziale di apprendimento che controlla quanto modificare il modello in risposta all'erorore stimato, ogni volta che i pesi degli \"archi\" della rete vengono aggiornati.  \n",
    "  \n",
    "Per quanto riguarda la funzione di attivazione utilizzata dagli *hidden layers* è stata scelta la funzione `tanh` poichè, come visto in [letteratura](https://medium.com/codex/single-layer-perceptron-and-activation-function-b6b74b4aae66) risultati migliori in ambito di classificazione binaria.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'hidden_layer_sizes':[500, 100], 'alpha': [0.05, 0.1], 'learning_rate_init':[0.001, 0.05, 0.1]}\n",
    "\n",
    "mlp = MLPClassifier(activation='tanh', solver='sgd')\n",
    "gscv_mlp = GridSearchCV(mlp, param_grid=params, scoring='recall', cv=5, refit=True, n_jobs=-1)\n",
    "\n",
    "gscv_mlp.fit(trainX.fillna(-1), trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla fase di *Model Selection* risulta che una rete di dimensioni maggiori di quella proposta di default, $100$ neuroni nell'hidden layer, offra prestazioni maggiori in termini di `recall`. Infine, viene restituito come tasso iniziale di apprendimento migliore il valore $0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Estimator: \",gscv_mlp.best_estimator_)\n",
    "print(\"Best Params: \", gscv_mlp.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlp = MLPClassifier(hidden_layer_sizes=500, alpha=0.05, max_iter=1000, solver='sgd', learning_rate_init=0.1)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, model=mlp, model_name=\"MLP Classifier (1)\", experiment = \"init=0.1\")\n",
    "scores_mlp = pd.DataFrame(scores_dict).T\n",
    "scores_mlp\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo osservare, i parametri ricavati dalla model selection portano + il modello a classificare tutti gli elementi come appartenenti alla classe `survived`, e di conseguenza la recall per tale classe sarà massima, pari a $1.0$.\n",
    "\n",
    "La model selection è stata quindi effettuata con un approccio sperimentale testando diversi valori del `learning_rate`. In particolare si è effettuato l'addestramento della rete neurale con un valore molto basso di `learning_rate_init`$=0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "mlp = MLPClassifier(hidden_layer_sizes=500, alpha=0.05, max_iter=1000, solver='sgd', learning_rate_init=0.0001)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, model=mlp, model_name=\"MLP Classifier (2)\", experiment = \"init=0.0001\")\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "scores_mlp2 = pd.DataFrame(scores_dict).T\n",
    "scores_mlp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel dettaglio, sono stati analizzati i risultati ottenuti dai seguenti modelli:\n",
    "- Logistic Regression Classifier\n",
    "- Decision Tree Classifier\n",
    "- Gaussian Naive Bayes Classifier\n",
    "- Random Forest Classifier\n",
    "- AdaBoost Classifier\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "Di seguito i risultati di diversi classificatori applicati sul training set dopo il processo di feature tweaking, senza considerare l'imputazione dei dati, la PCA e il sampling, tecniche che peggiorano gli score causando perdita di informazioni significative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(solver=\"liblinear\")\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=lrc, model_name=\"Logistic Regression Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lrc.coef_[0]\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=dtc, model_name=\"Decision Tree Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = dtc.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=dtc, model_name=\"Gaussian Naive Bayes Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il GaussianNB non offre un metodo intrinseco per valutare l'importanza delle caratteristiche. I metodi Naive Bayes funzionano determinando le probabilità condizionali e prior associate alle features, e predicono la classe utilizzando la probabilità più alta. Pertanto, non ci sono coefficienti calcolati o associati alle features utilizzate per addestrare il modello.\n",
    "\n",
    "Detto questo, ci sono metodi che puoi applicare post-hoc per analizzare il modello dopo che è stato addestrato. Uno di questi metodi è l'importanza della permutazione e, convenientemente, è stato implementato anche in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "importance = permutation_importance(gnb, testX, testY).importances_mean\n",
    "print(importance)\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=rfc, model_name=\"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rfc.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=abc, model_name=\"AdaBoost Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = abc.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=rfc, model_name=\"Linear Discriminant Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "trainX_scaled = pd.DataFrame(scaler.fit_transform(trainX.fillna(-1)),columns = trainX.columns)\n",
    "testX_scaled = pd.DataFrame(scaler.transform(testX.fillna(-1)),columns = testX.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='sigmoid')\n",
    "run_classifications(trainX_scaled, trainY, testX_scaled, testY, model=svc_classifier, model_name='SVC_SIGMOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = svc.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc = KNeighborsClassifier(n_neighbors=5)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=knc, model_name=\"KNeighbors Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = knc.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "run_classifications(trainX,trainY,testX,testY, fill = True, \n",
    "        model=mlp, model_name=\"MLP Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = mlp.feature_importances_\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "plt.bar(trainX.columns, importance)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come atteso, le reti neruali offrono ottimi risultati se utilizzate su grandi quantità di dati e sopratutto se addestrate continuamente nel tempo. Nel nostro caso, non avendo a disposizione una capacità computazionale elevata, abbiam dovuto limitare il numero di volte cui ogni data point viene utilizzato, `max_iter`, non giungendo dunque ad una convergenza assoluta sulla classificazione.  \n",
    "\n",
    "Inoltre, il numero di hidden layer e la loro dimensione dipende anche dalle caratteristiche della macchina che effettua l'addestramento. Ciò non ci ha permesso di aumentare eccessivamente la dimensione della rete, poichè avrebbe richiesto un tempo di addestramento troppo elevato. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confronto e Conclusioni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd33f47137b00f9a3c223ad7cd7ea8009173bfffa8c9627ff61bae578e5b56e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
